{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6de11f39-18b0-43bf-a978-4ac8b6496c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../py-irt')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import jsonlines\n",
    "import cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b92d9fe4-dbd4-4d24-b1ec-c9ebadc89f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, n_problems):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        # hidden_dim is the (arbitrary) dimensionality of the hidden layers.\n",
    "        #\n",
    "        # If all you wanted to do was to to encode the current answer into the hidden layer output of each step,\n",
    "        # you could use a single hidden dimension, with 0 = wrong and 1 = right.  But that's not the only purpose of thie hidden layer.\n",
    "        # You also need to retain information about not just this answer, but past right or wrong answers, to help predict the next one.\n",
    "        #\n",
    "        # I was using hidden layers for output (one of the layers is supposed to keep track of \"will the student get the right answer?\"\n",
    "        # But I am not sure whether that will work.  The place I got this from uses a fully connected layer after the hidden layers,\n",
    "        # which can take the hidden layer output and turn it into the actual answers.\n",
    "        #\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # n_layers is the (arbitrary) number of hidden layers\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Defining the layers\n",
    "        # RNN Layer\n",
    "        #\n",
    "        # input_size = the number of features in the input.  This equals the number of problems times two, because\n",
    "        # (1) the problems are one-hot encoded and so the number of features required to describe each problem equals\n",
    "        # the number of problems, (2) you also need to know whether the student got the problem right.\n",
    "        # The minimal encoding is actually log_2 N_features + 1; that is, if problem five is encoded [1,0,1], and the\n",
    "        # student got it right (1), you would have [1, 0, 1, 1], which is (log_2 8) + 1.  But this apparently\n",
    "        # will not result in a good outcome.\n",
    "        #\n",
    "        # hidden_dim = the dimensionality of each hidden layer, which is arbitrary (more dimensions = more latent variables)\n",
    "        #\n",
    "        # n_layers = the number of hidden layers, which is arbitrary (more layers = more processing of latent variables)\n",
    "        #\n",
    "        # batch_first = True: input and output tensors are provided as (batch_number, problem_index, feature)\n",
    "        # Here, for input, feature corresponds to a one-hot encoding of the problems + answers, where (1, 0, 0, ...) means\n",
    "        # the student answered the first problem (1) and got it wrong (0).  And (0, 0, 1, 1, 0, ...) means the student answered\n",
    "        # the second problem (1) and got it right (1).\n",
    "        # In other words, for each batch and each problem in each batch, we need to input 2N values to one-hot encode the problems.\n",
    "        # Each batch is a student.\n",
    "        #\n",
    "        # For output, features corresponds to a hidden layer feature, I think.  What we need to do is to have a separate fully\n",
    "        # connected layer for each problem (but it's the same for all batches) that outputs a 1 dimensional result, which says\n",
    "        # whether the next problem is predicted to be gotten right or wrong.\n",
    "        #\n",
    "        # batch_number is the batch number (each batch is a student; we are running multiple students through the RNN at once)\n",
    "        # and problem_index is the problem number.  (Called seq in the documentation).  Since the feature of an input problem _is_\n",
    "        # the problem number, the input will look like:\n",
    "        # [[1, ans1, 0, 0, 0, 0, ...],\n",
    "        #  [0, 0, 1, ans2, 0, 0, ...],\n",
    "        #  [0, 0, 0, 0, 1, ans3, ...]]\n",
    "        # The output, in contrast, is not like this, because the output features use the hidden layer rather than one-hot.\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        # Fully connected layers.  We need one for each problem.\n",
    "        self.fc_list = list()\n",
    "        for n in range(n_problems):\n",
    "            self.fc_list.append(torch.nn.Linear(hidden_dim, 1))\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        # If batch_first = True, the input tensor is of the form (num_students, problem_index, one_hot_feature), see above\n",
    "        # So input_data.size(0) must be the number of batches, i.e. students or batches.\n",
    "        n_students = input_data.size(0)\n",
    "        n_problems = input_data.size(1)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        # We initialize one set of hidden layers for each batch (student),\n",
    "        # so the dimensionality is (num_layers, batch_size, arbitrary_hidden_dim)\n",
    "        # We don't need to pass n_layers or hidden_dim to init_hidden, because those are member variables of self from __init__.\n",
    "        # Note: the hidden layer _always_ starts the forward pass in a particular (zeroed) state!  We do need to know the student's\n",
    "        # answers, but these are not part of the initial state.  Rather, they are part of the input features.\n",
    "        hidden_0 = self.init_hidden(n_students)\n",
    "\n",
    "        # Passing in the input data and initial hidden state into the model and obtaining outputs\n",
    "        # \"hidden_0\" is the initial hidden state in the form (num_layers, n_students, arbitrary_hidden_dim)\n",
    "        # \"hidden_out\" is the output hidden state at the end, including all layers of hidden state.\n",
    "        # \"out\" is of shape (n_students, n_problems, arbitrary_hidden_dim).  It shows the last hidden layer at each time step t.\n",
    "        # We will use each fc layer to turn each problem's hidden layer into the guess as to correctness, which can be checked later.\n",
    "        out, hidden_out = self.rnn(input_data, hidden_0)\n",
    "\n",
    "        whole_list = list()\n",
    "        # If I create whole_list piecemeal in this way, will the requires_grad allow propagation?\n",
    "        # I don't see that I have any alternative -- it seems to me that I need separate fully-connected layers for each problem.\n",
    "        # For example, what if different problems belong to different categories, and I must treat these categories differenlty?\n",
    "        for ns in range(n_students):\n",
    "            student_list = list() # list for this student\n",
    "            student_list.append(torch.tensor(0.5, dtype=torch.float32, requires_grad=True)) # We cannot predict the first problem, because there is no prior information\n",
    "            for np in range(1, n_problems): # We predict the 2nd through the last problem.\n",
    "                prediction = self.fc_list[np](out[ns, np, :]) # we use the hidden layer to calculate the likelihood of correctness.\n",
    "                # self.fc_list[0] is unused, because we cannot predict the first problem.\n",
    "                # self.fc_list is a list, because each problem must use the hidden layer to calculate correctness in a different way.\n",
    "                student_list.append(prediction[0]) # prediction is an array of length 1, so its 0th element is its only element.\n",
    "            whole_list.append(torch.stack(student_list))\n",
    "        \n",
    "        return torch.stack(whole_list) # this array has shape (n_students, n_problems), just like make target produces\n",
    "    \n",
    "    def init_hidden(self, n_students):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, n_students, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8d89bf84-ae43-4d6c-9d3a-d7ad7c80cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not available, CPU used\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ec047a51-1c58-4a69-bb91-3ebbf63343af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_val(i: int, j: int, student_skill: np.array, problem_difficulty: np.array):\n",
    "    \"\"\"i: index of student\n",
    "       j: index of problem\n",
    "       student_skill: 1D array of student skills\n",
    "       problem_difficulty: 1D array of problem difficulties\n",
    "    \"\"\"\n",
    "    prob_guess = 0.2\n",
    "    prob = prob_guess + (1 - prob_guess) / (1 + np.exp(student_skill[int(i)] - problem_difficulty[int(j)]))\n",
    "    return np.random.choice(2, 1, p=[1 - prob, prob])[0]  \n",
    "\n",
    "def irt_func(m1: np.array, m2: np.array, student_skill: np.array, problem_difficulty: np.array):\n",
    "    \"\"\"m1: 2D matrix where each entry is a student index\n",
    "       m2: 2D matrix where each entry is a problem index\n",
    "       student_skill: 1D array of student skills\n",
    "       problem_difficulty: 1D array of problem difficulties\n",
    "    \"\"\"\n",
    "    func = np.vectorize(lambda i, j: rand_val(i, j, student_skill, problem_difficulty))\n",
    "    return func(m1, m2)\n",
    "\n",
    "\n",
    "class RNN_IRT:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_students_train = 10000\n",
    "        self.n_students_val = 1000\n",
    "        self.n_problems = 10\n",
    "        self.n_hidden = 2\n",
    "        self.problem_difficulty = np.random.uniform(low = 0, high = 1, size=(self.n_problems,))\n",
    "\n",
    "    # make_target produces a rank 2 tensor that has dimension (n_students, n_problems)\n",
    "    # it's the students' actual responses (or probabilities thereof) to be compared with the predictions\n",
    "    def make_target(self, n_students: int, n_problems: int, student_skill: np.array):\n",
    "        output_np = np.fromfunction(irt_func, (n_students, n_problems), dtype=float, student_skill=student_skill, problem_difficulty=self.problem_difficulty)\n",
    "        output = torch.from_numpy(output_np)\n",
    "        output = torch.tensor(output, dtype=torch.float32)\n",
    "        return output\n",
    "\n",
    "    # This produces a rank 3 tensor which has dimensions (n_students, n_problems, 2 * n_problems)\n",
    "    # That is: (students, problems, features of problems)\n",
    "    # It is passed the target, which was created by make_target and has dimensions (n_students, n_problems)\n",
    "    # The target has the same data, only in a different shape - we're converting it to a kind of one-hot encoding.\n",
    "    def make_input(self, n_students: int, n_problems: int, student_skill, target):\n",
    "        input = list()\n",
    "        for ns in range(n_students):\n",
    "            input_part = torch.zeros(n_problems, 2 * n_problems)\n",
    "            for np in range(n_problems):\n",
    "                input_part[np, 2 * np] = 1\n",
    "                input_part[np, 2 * np + 1] = target[ns, np]\n",
    "            input.append(input_part)\n",
    "        return torch.stack(input)\n",
    "    \n",
    "    def make_data(self):\n",
    "        self.student_skill_train = np.random.uniform(low = 0, high = 1, size=(self.n_students_train,))\n",
    "        self.student_skill_val = np.random.uniform(low = 0, high = 1, size=(self.n_students_val,))\n",
    "        self.train_target = self.make_target(self.n_students_train, self.n_problems, self.student_skill_train)\n",
    "        self.train_input = self.make_input(self.n_students_train, self.n_problems, self.student_skill_train, self.train_target)\n",
    "        self.val_target = self.make_target(self.n_students_val, self.n_problems, self.student_skill_val)\n",
    "        self.val_input = self.make_input(self.n_students_val, self.n_problems, self.student_skill_val, self.val_target)\n",
    "\n",
    "    def make_target_jsonlines_for_irt(self):\n",
    "        print(\"Now making target jsonlines file.\")\n",
    "        target_data = self.make_target(self.n_students_train, self.n_problems, self.student_skill_train)\n",
    "        with jsonlines.open('rnn_irt.jsonlines', mode='w') as writer:\n",
    "            for n, obj in enumerate(target_data):\n",
    "                writer.write({\"subject_id\": str(n), \"responses\": {k: int(obj[k]) for k in range(len(obj))}})\n",
    "\n",
    "    def train_evaluate_compute_mse_irt(self):\n",
    "        print(\"Now training and evaluating using IRT.\")\n",
    "        cli.train_and_evaluate(\"1pl\", \"rnn_irt.jsonlines\", \"../py-irt/out\")\n",
    "        loss_sum = list()\n",
    "        print(\"Now computing MSE from model predictions jsonlines file.\")\n",
    "        with jsonlines.open('../py-irt/out/model_predictions.jsonlines', mode='r') as reader:\n",
    "            for obj in reader:\n",
    "                loss_sum.append((obj['response'] - obj['prediction'])**2)\n",
    "        mse_for_irt = sum(loss_sum) / len(loss_sum)\n",
    "        print(\"MSE for IRT = \", mse_for_irt)\n",
    "        return mse_for_irt\n",
    "\n",
    "\n",
    "    def training_loop(self, n_epochs: int, device, model: Model, loss_fn, optimizer: torch.optim.Optimizer):\n",
    "        self.make_data()\n",
    "        self.train_input = self.train_input.to(device)\n",
    "        self.train_target = self.train_target.to(device)\n",
    "        self.val_input = self.val_input.to(device)\n",
    "        self.val_target = self.val_target.to(device)\n",
    "        \n",
    "        # x = input()\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            model.train()\n",
    "            train_predictions = model(self.train_input)\n",
    "            train_loss = loss_fn(train_predictions, self.train_target)\n",
    "\n",
    "            #print(\"train_predictions:\")\n",
    "            #print(train_predictions)\n",
    "            # x = input()\n",
    "            optimizer.zero_grad() # Clear gradients\n",
    "            train_loss.backward() # Back propagation\n",
    "            #for name, param in model.named_parameters():\n",
    "            #    print(f\"Parameter: {name}, Gradient: {param.grad}\")\n",
    "            optimizer.step() # Update weights\n",
    "            \n",
    "            if epoch%10 == 0 or epoch == 1:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    val_predictions = model(self.val_input)\n",
    "                    val_loss = loss_fn(val_predictions, self.val_target)\n",
    "                    print(f'Epoch {epoch} out of {n_epochs}')\n",
    "                    print(f\"Val loss: {val_loss.item():.4f}\")\n",
    "                    print(f\"Train loss: {train_loss.item():.4f}\")\n",
    "\n",
    "    def train_rnn(self):\n",
    "        device = get_device()\n",
    "        ### self.make_data()\n",
    "        # Instantiate the model with hyperparameters\n",
    "        # input_size is the number of features; here, it is twice n_problems because \n",
    "        # the input is one-hot encoded, with one feature per problem and.\n",
    "        # hidden_dim is the dimension of the hidden layer, which is an arbitrary number.\n",
    "        # We want the 0th dimension of the hidden layer to be the output; the other dimensions keep track of latent variables.\n",
    "        model = Model(input_size=self.n_problems * 2, hidden_dim=self.n_hidden, n_layers=1, n_problems=self.n_problems)\n",
    "        # We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "        model.to(device)\n",
    "        \n",
    "        n_epochs = 1000\n",
    "        lr=0.1\n",
    "        \n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "        self.training_loop(\n",
    "            n_epochs = n_epochs,\n",
    "            device = device,\n",
    "            model = model,\n",
    "            loss_fn = loss_fn,\n",
    "            optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85b4e2-8e37-4e6c-a8be-cf3a6c95af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the idea for the code is based on this website:\n",
    "# https://web.archive.org/web/20231223192939/https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "448d5d70-2a52-4448-9ac2-5593d1113a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_irt = RNN_IRT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84f6507b-0ca2-41b7-b7f9-f9792144a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "rnn_irt.make_target_jsonlines_for_irt()\n",
    "mse_for_irt = rnn_irt.train_evaluate_compute_mse_irt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2014ab16-1a80-419e-91f8-e646c6d1706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2593336102663726\n"
     ]
    }
   ],
   "source": [
    "print(mse_for_irt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "84935f4e-9350-4711-92b6-d3f164134380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/w7l8k8x95msf7xpymhmw0vj40000gn/T/ipykernel_1472/617757952.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = torch.tensor(output, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 1000\n",
      "Val loss: 0.7321\n",
      "Train loss: 0.8866\n",
      "Epoch 10 out of 1000\n",
      "Val loss: 0.2602\n",
      "Train loss: 0.2704\n",
      "Epoch 20 out of 1000\n",
      "Val loss: 0.2123\n",
      "Train loss: 0.2150\n",
      "Epoch 30 out of 1000\n",
      "Val loss: 0.2017\n",
      "Train loss: 0.2034\n",
      "Epoch 40 out of 1000\n",
      "Val loss: 0.1970\n",
      "Train loss: 0.1988\n",
      "Epoch 50 out of 1000\n",
      "Val loss: 0.1939\n",
      "Train loss: 0.1952\n",
      "Epoch 60 out of 1000\n",
      "Val loss: 0.1929\n",
      "Train loss: 0.1942\n",
      "Epoch 70 out of 1000\n",
      "Val loss: 0.1915\n",
      "Train loss: 0.1928\n",
      "Epoch 80 out of 1000\n",
      "Val loss: 0.1899\n",
      "Train loss: 0.1912\n",
      "Epoch 90 out of 1000\n",
      "Val loss: 0.1883\n",
      "Train loss: 0.1896\n",
      "Epoch 100 out of 1000\n",
      "Val loss: 0.1872\n",
      "Train loss: 0.1884\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[195], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrnn_irt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[193], line 129\u001b[0m, in \u001b[0;36mRNN_IRT.train_rnn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m    127\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[193], line 97\u001b[0m, in \u001b[0;36mRNN_IRT.training_loop\u001b[0;34m(self, n_epochs, device, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#print(\"train_predictions:\")\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#print(train_predictions)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# x = input()\u001b[39;00m\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Back propagation\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#for name, param in model.named_parameters():\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m#    print(f\"Parameter: {name}, Gradient: {param.grad}\")\u001b[39;00m\n\u001b[1;32m    100\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update weights\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_irt.train_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914dd8a8-b4ab-4ee2-9007-a7b7ea70f028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
